<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  
  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Domain adaptation in transformer models">
  <meta property="og:description" content="Description of domain adaptation techniques">
  <meta property="og:image" content="/assets/images/profile-pic.jpg">

  <title>Domain adaptation in transformer models</title>
  <meta name="description" content="Description of domain adaptation techniques">

  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.10.0/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>

<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

  <a class="navbar-brand" href="/"><h5><b>Nikhil Salodkar</b></h5></a>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-1x fa-bars text-themed"></i>
  </button>

  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav ml-auto"><a class="nav-item nav-link " href="/about/">About</a>

      <a class="nav-item nav-link " href="/projects/">Projects</a>

      <a class="nav-item nav-link " href="/services/">My Services</a>

      <a class="nav-item nav-link active" href="/blog/">Blog</a>

      <a class="nav-item nav-link " href="/contact/">Contact</a>

      

      <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
    </div>
  </div>

</nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1><b>Domain adaptation in transformer models</b></h1>

<p class="post-metadata text-muted">
  27 December 2022 -  
  <b>7 mins read time</b>

  <br>Tags: 
    
    <a class="text-decoration-none no-underline" href="/blog/tags#transformer-architectures">
      <span class="tag badge badge-pill text-primary border border-primary">Transformer Architectures</span>
    </a>
    
    <a class="text-decoration-none no-underline" href="/blog/tags#nlp">
      <span class="tag badge badge-pill text-primary border border-primary">NLP</span>
    </a>
    </p>

<p>Transformer models have proven to be quite performative and useful when correctly pre-trained on large text corpus 
and fine-tuned with small to moderately sized supervised datasets to solve downstream NLP tasks. Much of the research
in pre-training of Language models, and testing performance is done using general text corpus like Wikipedia, News
articles, Reddit comments and mixture of such types of text corpus sources. The main reason is easier availability 
of such resources and trying to acquire as much text and as varied text as possible for research purposes. So, many
of the popular pre-trained models turned out to be pre-trained using general purpose text which cannot necessarily
be categorized as a particular domain.</p>

<p>But, in real life and most of the production scenarios applications are made to handle a particular domain of text
data. For example, application to summarize news articles, application to perform search over scientific research
documents, chatbot built to answer education related queries and so on. Thus, these specialized applications focus
on particular domains of their own and, it begs the question, whether using a general purpose trained language
model to train for such specialized domain is the best thing to do. This question led to the research community in
seeking the area of domain adaptation for language models.</p>

<p>In this post we will understand what domain adaptation is, its benefits and how it can be done.</p>

<h2 id="what-is-domain-adaptation-and-why-it-is-important">What is Domain Adaptation and why it is important?</h2>
<p>Domain adaptation can be defined as any training technique which can help a pre-trained Language model to better
adapt itself to solve tasks in a specialized NLP text area.</p>

<p>This can be important because in real world there are many NLP domains with common tasks which can benefit if 
the language model is adapted suitably. Some common NLP domains and popular tasks are as given below:</p>
<ul>
  <li><strong>Medical and/or Pharmaceutical domain</strong> : Popular tasks and applications in this domain include
    <ul>
      <li>Entity detection and Linking: Detecting special pharmaceutical and medical entities in a text like entities
pertaining to diseases, medicine, proteins, symptoms, side effects etc.</li>
      <li>Specialized chatbots for answering patient queries and return correct search results</li>
      <li>Research paper recommendation: What research to read next pertaining to research area of interest</li>
      <li>Extractive text summarization: Automatically highlighting important sentences in medical research papers etc</li>
    </ul>
  </li>
  <li><strong>General News domain</strong> :
    <ul>
      <li>News article summarization</li>
      <li>Article recommendation based on readers interest and past history</li>
      <li>Clustering news articles and finding similar or correlated news</li>
    </ul>
  </li>
  <li><strong>Specialized Product sales and services</strong> :
    <ul>
      <li>Sentiment analysis on reviews of purchased products like retail products or even watched movies or games played</li>
      <li>Customer support service assistance for faster solution resolution</li>
      <li>Intelligent IVR systems</li>
    </ul>
  </li>
</ul>

<p>Many NLP applications are focused on a particular industry and may also have focused subdomain in that industry,
like in Pharma industry language around clinical trials might be different to drug discovery research and very
different compared to clinical Electronic Medical records language. There could be difference in vocabulary,
sentence structure, short forms, language formality and so on. Thus, in most real world cases it may become
necessary to specialize language model to get better performance on interested tasks.</p>

<p>To create such specialized models one approach is to train the transformer based language models directly using
relevant domain text data. This has been done on some popular models like <strong>BioMegatron, Sci-Bert, BioBERT</strong> etc, which
were pre-trained directly on specialized text corpus like Pubmed extracts and other scientific journals. But, in
most cases for specialized domains there either would be dearth in quantity of text necessary to train a full-fledged
language model, or it would be too computationally expensive and time-consuming.</p>

<p>Thus, it becomes prudent that we find useful training methods that could help in adaptation of existing Language models
more efficiently.</p>

<h2 id="domain-adaptation-strategies">Domain adaptation strategies</h2>
<p>Let’s review some domain adaptation strategies that we can follow and discuss some research papers
proposing these ideas.</p>

<h4 id="adapting-domain-by-continuing-the-pre-training-using-in-domain-dataset">Adapting domain by continuing the pre-training using in-domain dataset</h4>
<p>This idea is intelligently introduced in the paper by Gururangan et al. [1] titled “Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks”
  where they investigated the performance improvement of existing Language model when <strong>“Domain adaptive pre-training” (DAPT)</strong> is
  performed in high and low-resource setting. They considered four domains - biomedical (BIOMED), computer science (CS), news and reviews
  and adapted the Roberta model.
  <strong>Some of the important observations were:</strong></p>
<ul>
  <li>DAPT improves over Roberta in all domains especially for BIOMED, CS, and REVIEWS where the target domain is more distant than ROBERTA’s
  source domain. Thus, the more unique the target domain, the more useful domain adaptation could be. This includes target
  domain’s difference in vocabulary as well as target domain’s difference in context and meaning.</li>
  <li>For very specialized domains it is sometimes very difficult to even get a large scale untagged text corpus. In such cases
  a technique called <strong>“Task adaptive pre-training (TAPT)”</strong> could be applied where pre-training is done on the unlabeled 
  training set itself which is later going to be used for supervised training. The TAPT approach uses a far smaller pre-training
  corpus, but one that is much more task-relevant. TAPT could be done on top of DAPT as well and is especially useful when
  task data is narrowly defined subset of a broader domain. <strong>Fig 1</strong> from the paper shows the summary of the improvements when 
  DAPT AND TAPT is performed on different datasets and tasks.</li>
</ul>

<table>
  <tbody>
    <tr>
      <td><img src="../assets/images/blogpost/domain_adaptation_results.png" alt="" /></td>
    </tr>
    <tr>
      <td><em>Fig 1. Different phases of pre-training results. Source: [1]</em></td>
    </tr>
  </tbody>
</table>

<h4 id="updating-tokenization-strategy-andor-the-vocabulary">Updating tokenization strategy and/or the vocabulary</h4>
<p>Another approach that could be relevant for adapting to a new domain can be adapting to a new vocabulary most pertinent
  to the task domain at hand. In the DAPT and TAPT discussed above the vocabulary was kept the same even though domain adapted.
  For many specialized domains vocabulary differs significantly compared to vocabulary of general domain. For example,
  consider this example <strong>Fig 2</strong> from paper [2] titled “AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain” where specialized
  domain can include complicated words which won’t be tokenized in efficient and meaningful manner using the general domain
  vocabulary.</p>

<table>
  <tbody>
    <tr>
      <td><img src="../assets/images/blogpost/special_tokenization.png" alt="" /></td>
    </tr>
    <tr>
      <td><em>Fig 2. Special tokenization example Source: [2]</em></td>
    </tr>
  </tbody>
</table>

<p>As seen above, the complex word corticosterone is tokenized is much efficient manner and its sub tokens can be more meaningful
  compared to when done using default general domain tokenization.
  The paper [2] showcases strategies for fine-tuning with regularization to adapt and add specialized tokenization which
  could result in better performance of Language models.</p>

<p>Another tokenization and vocabulary adapting approach is outlined in paper [3] titled “INDOBERTWEET: A Pretrained Language Model for Indonesian Twitter
  with Effective Domain-Specific Vocabulary Initialization”, where they show innovative technique of initializing domain-specific
  vocabulary to improve pretraining from scratch using resource limited corpus.
  They concluded that using the averaging of subword embeddings is more effective tokenization initializing technique for 
  newly formed more holistic tokens in specialized domains which reduces the overhead of domain-adaptive pre-training by 80% compared
  to previously token embeddings initializing methods.</p>

<h3 id="final-thoughts">Final thoughts</h3>
<p>Whenever we face situations where we want to use an existing language model to solve a niche task in a particular specialized domain,
we should keep in mind that we can use domain adaptation techniques and tokenization and/or vocabulary adaptation techniques
to improve the performance of the models.</p>

<h3 id="references">References</h3>
<ol>
  <li><a href="https://arxiv.org/pdf/2004.10964.pdf">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a></li>
  <li><a href="https://arxiv.org/pdf/2110.13434.pdf">AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain</a></li>
  <li><a href="https://arxiv.org/pdf/2109.04607.pdf">INDOBERTWEET: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization</a></li>
</ol>



<div class="pt-5">
  
</div>

</div>
  </main>
  <footer class="mt-auto py-3 text-center">

<!--  <small class="text-muted mb-2">-->
<!--    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>-->
<!--    by <strong>Nikhil Salodkar</strong>-->
<!--  </small>-->

  <div class="container-fluid justify-content-center"><a class="social mx-1"  href="https://www.github.com/nikhil-salodkar"
       style="color: #6c757d"
       onMouseOver="this.style.color='#333333'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-github fa-1x"></i>
    </a><a class="social mx-1"  href="https://www.linkedin.com/in/nikhil-salodkar-90"
       style="color: #6c757d"
       onMouseOver="this.style.color='#007bb5'"
       onMouseOut="this.style.color='#6c757d'">
      <i class="fab fa-linkedin-in fa-1x"></i>
    </a>

</div><!--  <small id="attribution">-->
<!--    theme <a href="https://github.com/YoussefRaafatNasry/portfolYOU">portfolYOU</a>-->
<!--  </small>-->

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>
</body>

</html>